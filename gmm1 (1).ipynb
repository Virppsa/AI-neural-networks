{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install openimages\n",
    "!pip install --upgrade pip\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install glob\n",
    "!pip install PIL\n",
    "!pip install numpy\n",
    "!pip install itertools"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import itertools \n",
    "import os\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.transform import resize\n",
    "from openimages.download import download_dataset\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from openimages.download import download_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "#  Atsiunciame paveikslelius\n",
    "classes = [\"Cat\", \"Dog\", \"Lamp\"]\n",
    "data_directory = \"data\"\n",
    "classes_file_path = \"data/classes.txt\"\n",
    "number_for_samples = 300\n",
    "download_dataset(data_directory, classes, limit=number_for_samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Su class file \n",
    "with open(classes_file_path) as cf:\n",
    "  class_list = [line.split('\\n')[0].lower().split(\" \") for line in cf.readlines()] #pasplitinam tas klases, nes kaikurios yra iš dviejų žodžių, kad galimą būtų sudėti į array patogiau\n",
    "\n",
    "class_indexes = { c: [idx for idx, s in enumerate(class_list) if c.lower() in s][0] for c in classes } #Paprašome, kad surastų kur prasideda kiekvienos atskiros klasės indeksas\n",
    "print(class_indexes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Cat': 282, 'Dog': 151, 'Lamp': 846}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "weights = ResNet152_Weights.DEFAULT\n",
    "model = resnet152(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "preprocess = weights.transforms()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "class CustomLab1Dataset(Dataset):\n",
    "    def __init__(self, images_dir, preprocess):\n",
    "        self.images_dir = images_dir\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        self.class1_files = glob.glob(self.images_dir + \"/{}/images/*.jpg\".format(classes[0].lower()))\n",
    "        self.class2_files = glob.glob(self.images_dir + \"/{}/images/*.jpg\".format(classes[1].lower()))\n",
    "        self.class3_files = glob.glob(self.images_dir + \"/{}/images/*.jpg\".format(classes[2].lower()))\n",
    "\n",
    "        self.class1 = len(self.class1_files)\n",
    "        self.class2 = len(self.class2_files)\n",
    "        self.class3 = len(self.class3_files)\n",
    "\n",
    "        self.files = self.class1_files + self.class2_files + self.class3_files\n",
    "\n",
    "        self.labels = np.zeros(len(self.files))\n",
    "        self.labels[self.class1:self.class1+self.class2] = 1\n",
    "        self.labels[self.class1+self.class2:] = 2\n",
    "\n",
    "        self.order =  [x for x in np.random.permutation(len(self.labels))]\n",
    "        self.files = [self.files[x] for x in self.order]\n",
    "        self.labels = [self.labels[x] for x in self.order]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.files))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file = self.files[index]\n",
    "\n",
    "        image = Image.open(file).convert('RGB')\n",
    "        process_image = self.preprocess(image)\n",
    "\n",
    "        label = self.labels[index]\n",
    "        return (process_image, label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "dataset = CustomLab1Dataset(\"./data\", preprocess)\n",
    "dataLoader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=0, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "predictions, actual_values = {c: [] for c in classes}, {c: [] for c in classes}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #pas mane su  CPU, bet per google colab veiktų\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataLoader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).softmax(dim=1)\n",
    "\n",
    "        \n",
    "        for idx, c in enumerate(classes):\n",
    "            class_probabilities = outputs[:, class_indexes[c]].detach().cpu().numpy()\n",
    "            predictions[c].extend(class_probabilities)\n",
    "            actual_values[c].extend(labels.cpu().numpy() == idx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model_performance(predicted_scores, true_labels, threshold=0.5):\n",
    "\n",
    "    binary_predictions = (predicted_scores >= threshold).astype(int)\n",
    "\n",
    "    confusion_mtx = confusion_matrix(true_labels, binary_predictions)\n",
    "    \n",
    "    tp = confusion_mtx[1, 1]\n",
    "    tn = confusion_mtx[0, 0]\n",
    "    fp = confusion_mtx[0, 1]\n",
    "    fn = confusion_mtx[1, 0]\n",
    "\n",
    "    print(f\"True Positives (TP): {tp}, True Negatives (TN): {tn}, False Positives (FP): {fp}, False Negatives (FN): {fn}\")\n",
    "\n",
    "    # Dedu į biblioteką, kaip jūs rekomendavote\n",
    "    performance_metrics = {}\n",
    "    performance_metrics[\"accuracy\"] = (tp + tn) / (tp + tn + fp + fn)\n",
    "    performance_metrics[\"sensitivity\"] = tp / (tp + fn) # Recall\n",
    "    performance_metrics[\"precision\"] = tp / (tp + fp)\n",
    "    performance_metrics[\"F1\"] = 2 * (performance_metrics[\"precision\"] * performance_metrics[\"sensitivity\"]) / (performance_metrics[\"precision\"] + performance_metrics[\"sensitivity\"])\n",
    "\n",
    "    # Display classification report for further insights\n",
    "    print(\"\\nResults:\")\n",
    "    print(classification_report(true_labels, binary_predictions))\n",
    "\n",
    "    return performance_metrics\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "performance_summary = {}\n",
    "performance_summary['average'] = {}\n",
    "for cl in classes:\n",
    "    print(f\"\\nEvaluating performance for '{cl}' class:\")\n",
    "    performance_summary[cl] = evaluate_model_performance(np.array(predictions[cl]), np.array(actual_values[cl]))\n",
    "\n",
    "    for metric, value in performance_summary[cl].items():\n",
    "        print(f\"{metric.capitalize()}: {value}\")\n",
    "        \n",
    "        if metric in performance_summary['average']:\n",
    "            performance_summary['average'][metric] += value\n",
    "        else:\n",
    "            performance_summary['average'][metric] = value\n",
    "\n",
    "\n",
    "for metric in performance_summary['average']:\n",
    "    performance_summary['average'][metric] /= len(classes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluating performance for class 'Cat':\n",
      "True Positives: 8, True Negatives: 600, False Positives: 0, False Negatives: 292\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      1.00      0.80       600\n",
      "        True       1.00      0.03      0.05       300\n",
      "\n",
      "    accuracy                           0.68       900\n",
      "   macro avg       0.84      0.51      0.43       900\n",
      "weighted avg       0.78      0.68      0.55       900\n",
      "\n",
      "Accuracy: 0.6755555555555556\n",
      "Sensitivity: 0.02666666666666667\n",
      "Precision: 1.0\n",
      "F1: 0.05194805194805195\n",
      "\n",
      "Evaluating performance for class 'Dog':\n",
      "True Positives: 3, True Negatives: 600, False Positives: 0, False Negatives: 297\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      1.00      0.80       600\n",
      "        True       1.00      0.01      0.02       300\n",
      "\n",
      "    accuracy                           0.67       900\n",
      "   macro avg       0.83      0.51      0.41       900\n",
      "weighted avg       0.78      0.67      0.54       900\n",
      "\n",
      "Accuracy: 0.67\n",
      "Sensitivity: 0.01\n",
      "Precision: 1.0\n",
      "F1: 0.019801980198019802\n",
      "\n",
      "Evaluating performance for class 'Lamp':\n",
      "True Positives: 29, True Negatives: 600, False Positives: 0, False Negatives: 271\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.69      1.00      0.82       600\n",
      "        True       1.00      0.10      0.18       300\n",
      "\n",
      "    accuracy                           0.70       900\n",
      "   macro avg       0.84      0.55      0.50       900\n",
      "weighted avg       0.79      0.70      0.60       900\n",
      "\n",
      "Accuracy: 0.6988888888888889\n",
      "Sensitivity: 0.09666666666666666\n",
      "Precision: 1.0\n",
      "F1: 0.1762917933130699\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def classify_image_from_url(image_url, model, preprocess):\n",
    "    \n",
    "    response = requests.get(image_url)\n",
    "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "    \n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # mini bečas\n",
    "\n",
    "    # pas mane defoltiniu pajungia ne su kuda o su mano CPU ir man neveikia modeliai su AMD plokšte, bet jei per google colab leisčiau - veiktų\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "    _, top_catid = torch.topk(probabilities, 1)\n",
    "\n",
    "    classifier_number = top_catid.item() #Paprašau rezultato indekso classes.txt faile\n",
    "\n",
    "    print(classifier_number)\n",
    "\n",
    "    if classifier_number >= class_indexes['Dog'] and classifier_number < class_indexes['Cat']:\n",
    "        return \"Dog\"\n",
    "    elif classifier_number >= class_indexes['Cat'] and classifier_number < 294: \n",
    "        return \"Cat\"\n",
    "    elif classifier_number == class_indexes['Lamp']:\n",
    "        return \"Lamp\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "image_url = \"https://www.theenglishhome.co.uk/wp-content/uploads/2023/10/Cathy-Nordstrom-x-Salig-Studio-Living-room-Florence-in-Tobacco-base-in-Mist-Mikael-Lundblad-Portrait.jpg\"\n",
    "classification_result = classify_image_from_url(image_url, model, preprocess)\n",
    "print(f\"The image is a {classification_result}.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "846\n",
      "The image is a Lamp.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "5e9c50075ae1428da95cb9ffbc54e77ae70a352308906d4e5967b0a282c866c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}